{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Семинар 12: парсинг и немного пиратства\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Почитать\n",
    "\n",
    "- азы всех азов: [статейка с хабра](https://habr.com/ru/company/ods/blog/346632/) \n",
    "- а парсинг это законно? [статейка с хабра](https://habr.com/ru/post/545818/)\n",
    "- telegram habr bot как идея для вашего проекта: [статейка с хабра](https://habr.com/ru/post/686174/) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Зачем собирать данные автоматически? \n",
    "\n",
    "- В общем случае чтобы вытащить информацию, которую собирать руками очень долго (данные о курсах валют, новостные ленты, таблицы с показателями)\n",
    "- Написать классного телеграм бота с доставанием нужной вам информации\n",
    "- Забирать данные конкурентов, например, следить за ценами конкурентов и подстраивать свои цены под них\n",
    "- Собирать тексты, чтобы анализировать их с точки зрения лингвистики или обучать модели\n",
    "- ... и вы наверняка вспомните случаи когда парсинг вам очень помог бы\n",
    "\n",
    "<br>\n",
    "\n",
    "<br>\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/hse-econ-data-science/eds_spring_2020/master/sem05_parsing/image/aaaaaa.png\" width=\"500\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что такое HTML? \n",
    "\n",
    "**HTML (HyperText Markup Language)**  — это такой же язык разметки как Markdown или LaTeX. Он является стандартным для написания различных сайтов. Команды в таком языке называются **тегами**. Если открыть абсолютно любой сайт, нажать на правую кнопку мышки, а после нажать `View page source`, то перед вами предстанет HTML скелет этого сайта.\n",
    "\n",
    "Давайте попробуем зайти на любую страницу, открыть ее код и немного изменить заголовок или текст новостей. Теперь становится понятней откуда берутся многие фейки.\n",
    "\n",
    "Понятное дело, что вы так меняете html-страничку только на своём компьютере (клиенте). На сервере остаётся исходная версия. Когда вы обновите страничку все правки исчезнут. \n",
    "\n",
    "HTML-страница это ни что иное, как набор вложенных тегов. Можно заметить, например, следующие теги:\n",
    "\n",
    "- `<title>` – заголовок страницы\n",
    "- `<h1>…<h6>` – заголовки разных уровней\n",
    "- `<p>` – абзац (paragraph)\n",
    "- `<div>` – выделения фрагмента документа с целью изменения вида содержимого\n",
    "- `<table>` – прорисовка таблицы \n",
    "- `<tr>` – разделитель для строк в таблице \n",
    "- `<td>` – разделитель для столбцов в таблице\n",
    "- `<b>` – устанавливает жирное начертание шрифта\n",
    "\n",
    "Обычно команда `<...>` открывает тег, а  `</...>` закрывает его. Все, что находится между этими двумя командами, подчиняется правилу, которое диктует тег. Например, все, что находится между `<p>` и  `</p>` — это отдельный абзац.   \n",
    "\n",
    "Теги образуют своеобразное дерево с корнем в теге `<html>` и разбивают страницу на разные логические кусочки. У каждого тега есть свои потомки (дети) — те теги, которые вложены в него и свои родители. \n",
    "\n",
    "Например, HTML-древо страницы может выглядеть вот так:\n",
    "\n",
    "\n",
    "````\n",
    "<html>\n",
    "<head> Заголовок </head>\n",
    "<body>\n",
    "    <div>\n",
    "        Первый кусок текста со своими свойствами\n",
    "    </div>\n",
    "    <div>\n",
    "        Второй кусок текста\n",
    "            <b>\n",
    "                Третий, жирный кусок\n",
    "            </b>\n",
    "    </div>\n",
    "    Четвёртый кусок текста\n",
    "</body>\n",
    "</html>\n",
    "````\n",
    "\n",
    "Можно работать с этим html как с текстом, а можно как с деревом. Обход этого дерева и есть парсинг веб-страницы. Мы всего лишь будем находить нужные нам узлы среди всего этого разнообразия и забирать из них информацию!\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/hse-econ-data-science/eds_spring_2020/master/sem05_parsing/image/tree.png\" width=\"450\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Vanilla parser: Качаем цены на книги\n",
    "\n",
    "* Хотим собрать [цены на книги](http://books.toscrape.com)\n",
    "* Руками долго, напишем код на петухоне\n",
    "\n",
    "Доступ к веб-станицам позволяет получать модуль requests. Подгрузим его. Если у вас не установлен этот модуль, то придётся напрячься и установить:  `pip install requests`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests >> None # >> None во избежание кучи строк установки библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests  \n",
    "\n",
    "url = 'http://books.toscrape.com/catalogue/page-1.html'\n",
    "response = requests.get(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Благословенный 200 ответ - соединение установлено и данные получены, всё чудесно! Если попытаться перейти на несуществующую страницу, то можно получить, например, знаменитую ошибку 404."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "requests.get('http://books.toscrape.com/big_scholarship')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Внутри response лежит html-разметка странички, которую мы парсим. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response.text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит неудобоваримо, как насчет сварить из этого дела что-то покрасивее? Например, прекрасный суп.\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/hse-econ-data-science/eds_spring_2020/master/sem05_parsing/image/alisa.jpg\" height=\"200\" width=\"200\"> \n",
    "\n",
    "Пакет **[`bs4`](https://www.crummy.com/software/BeautifulSoup/)**, a.k.a **BeautifulSoup** был назван в честь стишка про красивый суп из Алисы в стране чудес. Эта совершенно волшебная библиотека, которая из сырого и необработанного HTML (или XML) кода страницы выдаст вам структурированный массив данных, по которому очень удобно искать необходимые теги, классы, атрибуты, тексты и прочие элементы веб страниц.\n",
    "\n",
    "> Пакет под названием `BeautifulSoup` — скорее всего, не то, что вам нужно. Это третья версия (*Beautiful Soup 3*), а мы будем использовать четвертую. Так что нам нужен пакет `beautifulsoup4`. Чтобы было совсем весело, при импорте нужно указывать другое название пакета — `bs4`, а импортировать функцию под названием `BeautifulSoup`. В общем, сначала легко запутаться, но эти трудности нужно преодолеть однажды, а потом будет проще."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# распарсили страничку в дерево \n",
    "tree = BeautifulSoup(response.content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Внутри переменной `tree` теперь лежит дерево из тегов, по которому мы можем совершенно спокойно бродить. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.title.text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно вытащить из того места, куда мы забрели, текст с помощью метода `text`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.html.head.title.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С текстом можно работать классическими питоновскими методами. Например, можно избавиться от лишних отступов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.html.head.title.text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Более того, зная адрес элемента, мы сразу можем найти его. Например, вот так в коде страницы мы можем найти где именно для каждой книги лежит основная информация. Видно, что она находится внутри тега `article`, для которого прописан класс `product_pod` (грубо говоря, в html класс задаёт оформление соотвествующего кусочка страницы). \n",
    "\n",
    "Вытащим инфу о книге из этого тега. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books = tree.find_all('article', {'class' : 'product_pod'})\n",
    "books[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученный после поиска объект также обладает структурой bs4. Поэтому можно продолжить искать нужные нам объекты уже в нём."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(books[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books[0].find('p', {'class': 'price_color'}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обратите внимание, что для поиска есть как минимум два метода: `find` и `find_all`. Если несколько элементов на странице обладают указанным адресом, то метод `find` вернёт только самый первый. Чтобы найти все элементы с таким адресом, нужно использовать метод `find_all`. На выход будет выдан список.\n",
    "\n",
    "Кроме содержимого у тегов часто есть атрибуты. Например, у названия книги есть атрибуты `title` и `href`: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books[0].h3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Их тоже можно вытащить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books[0].h3.a.get('href')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books[0].h3.a.get('title')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А ещё по этим атрибутам можно искать интересующие нас кусочки страницы. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.find_all('a', {'title': 'A Light in the Attic'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Собственно говоря, это всё. \n",
    "\n",
    "Обратите внимание, что на сайте все книги лежат на разных страничках. Если попробовать потыкать их, можно заметить, что в ссылке будет меняться атрибут `page`. Значит, если мы хотим собрать все книги, надо создать кучу ссылок с разным `page` внутри цикла. Когда качаешь данные с более сложных сайтов, в ссылке часто есть огромное количество атрибутов, которые регулируют выдачу.\n",
    "\n",
    "\n",
    "Давайте запишем весь код для сбора книг в виде функции. На вход она будет принимать номер странички, которую надо скачать. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page(p):\n",
    "    \n",
    "    # изготовили ссылку\n",
    "    url = f'http://books.toscrape.com/catalogue/page-{p}.html'\n",
    "    \n",
    "    # сходили по ней\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    # построили дерево \n",
    "    tree = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # нашли в нём всё самое интересное\n",
    "    books = tree.find_all('article', {'class' : 'product_pod'})\n",
    "    \n",
    "    infa = [ ]\n",
    "    \n",
    "    for book in books:\n",
    "        infa.append({'price': book.find('p', {'class': 'price_color'}).text,\n",
    "                     'href': book.h3.a.get('href'),\n",
    "                     'title': book.h3.a.get('title')})\n",
    "                     \n",
    "    return infa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось только пройтись по всем страничкам от page-1 до page-50 циклом и данные у нас в кармане. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm # удобная штука для отрисовки прогресс бара\n",
    "\n",
    "infa = []\n",
    "\n",
    "for p in tqdm(range(1,10)):\n",
    "    infa.extend(get_page(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда случаются такие объемы информации их зачастую удобнее воспринимать в виде таблички. В питоне для этого придумали целую библиотеку \n",
    "\n",
    "__Pandas (Python Data Analysis Library)__ — библиотека языка Python для удобных обработки и анализа данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(infa)\n",
    "print(df.shape) # размеры нашего датафрейма (число_строк, число_столбцов)\n",
    "df.head() # первые N строк (N=5 по дефолту, но в скобках можно указать нужное число строк df.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем немного поанализировать данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['price_float'] = df['price'].apply(lambda x: float(x.split('£')[1])) # приведем к числовому формату\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info() # получим общее описание"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe() # работает с числовыми переменными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Average price is:', df.price_float.mean())\n",
    "print('Min price is:', df.price_float.min())\n",
    "print('Max price is:', df.price_float.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.price_float < 12].title.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.title == 'The Project'].href.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим новую колонку полных ссылок\n",
    "df['full_link'] = 'http://books.toscrape.com/catalogue/' + df['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кстати говоря, если перейти по ссылке в саму книгу, там о ней будет куча дополнительной информации. Можно пройтись по всем ссылкам и выкачать себе по ним дополнительную информацию."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если на странице, которую вы спарсили, среди тэгов `<tr>` и `<td>` прячется таблица, чаще всего можно забрать её себе без написания цикла, который будет перебирать все стобцы и строки. Поможет в этом `pd.read_html`. Например, вот так можно забрать себе [табличку с сайта ЦБ](https://cbr.ru/currency_base/daily/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_html('https://cbr.ru/currency_base/daily/')[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Команда пытается собрать в массив все таблички c веб-страницы. Если хочется, можно сначала через bs4 найти нужную таблицу, а потом уже распарсить её:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get('https://cbr.ru/currency_base/daily/')\n",
    "tree = BeautifulSoup(resp.content, 'html.parser')\n",
    "\n",
    "# нашли табличку\n",
    "table = tree.find_all('table', {'class' : 'data'})[0]\n",
    "\n",
    "# распарсили её\n",
    "df = pd.read_html(str(table))[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А если вам интересно больше о работе в pandas, то можно почитать тут:\n",
    "- [10 minutes to pandas](https://pandas.pydata.org/docs/user_guide/10min.html)\n",
    "- [Pandas intro](https://khashtamov.com/ru/pandas-introduction/)\n",
    "- [Exploratory data analysis (EDA) using pandas intro](https://habr.com/ru/company/ruvds/blog/494720/)\n",
    "- [Pandas user guide](https://pandas.pydata.org/docs/user_guide/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Selenium\n",
    "\n",
    "Это инструмент для роботизированного управления браузером. Для его коректной работы нужно скачать драйвер: [для хрома](https://sites.google.com/a/chromium.org/chromedriver/downloads) или [для фаерфокса.](https://github.com/mozilla/geckodriver/releases) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# можно установить или указать путь если вы уже скачали драйвер на комп\n",
    "# https://www.selenium.dev/documentation/webdriver/getting_started/install_drivers/\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Передадим ссылку для перехода"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref = 'http://google.com'\n",
    "driver.get(ref)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдем место для ввода текста и установим туда курсор:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroka = driver.find_element_by_name(\"q\")\n",
    "stroka.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загуглим скандалы вышки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroka.send_keys('матфак вшэ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# находим кнопку для гугления и жмём её\n",
    "button = driver.find_element_by_name('btnK')\n",
    "button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заберем станицу в суп и возьмем все заголовки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = BeautifulSoup(driver.page_source)\n",
    "\n",
    "dirty_headings = bs.find_all('h3',attrs={'class':'LC20lb MBeuO DKV0Md'})\n",
    "clean_headings = [title.text for title in dirty_headings]\n",
    "clean_headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close() # не забываем закрыть браузер"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как еще искать элементы:\n",
    "\n",
    "- навести в коде страницы на нужный элемент и выбрать `Copy Xpath`\n",
    "- использовать код `driver.find_element_by_xpath('_____').click()`\n",
    "- гуглить все что хотите"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Селениум можно настроить так, чтобы физически браузер не открывался."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.selenium.dev/selenium/docs/api/javascript/module/selenium-webdriver/chrome_exports_Options.html\n",
    "options = webdriver.ChromeOptions()\n",
    "options.add_argument(\"headless\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(ChromeDriverManager().install(), options=options)\n",
    "\n",
    "ref = 'http://google.com'\n",
    "driver.get(ref)\n",
    "\n",
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Лайфхаки если сервер разозлился\n",
    "\n",
    "* Вы решили собрать себе немного данных \n",
    "* Сервер не в восторге от ковровой бомбардировки автоматическими запросами \n",
    "* Error 403, 404, 504, $\\ldots$ \n",
    "* Капча, требования зарегистрироваться\n",
    "* Заботливые сообщения, что с вашего устройства обнаружен подозрительный трафик\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/hse-econ-data-science/eds_spring_2020/master/sem05_parsing/image/doge.jpg\" width=\"450\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## а) быть терпеливым \n",
    "\n",
    "* Слишком частые запросы раздражают сервер\n",
    "* Ставьте между ними временные задержки "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(3) # и пусть весь мир подождёт 3 секунды"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## б) быть похожим на человека\n",
    "\n",
    "\n",
    "Запрос нормального человека через браузер выглядит так: \n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/hse-econ-data-science/eds_spring_2020/master/sem05_parsing/image/browser_get.png\" width=\"600\"> \n",
    "    \n",
    "С ним на сервер попадает куча информации! Запрос от питона выглядит так: \n",
    "\n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/hse-econ-data-science/eds_spring_2020/master/sem05_parsing/image/python_get.jpg\" width=\"250\"> \n",
    " \n",
    "Заметили разницу?  Очевидно, что нашему скромному запросу не тягаться с таким обилием мета-информации, которое передается при запросе из обычного браузера. К счастью, никто нам не мешает притвориться человечными и пустить пыль в глаза сервера при помощи генерации фейкового юзер-агента. Библиотек, которые справляются с такой задачей, существует очень и очень много, лично мне больше всего нравится [fake-useragent.](https://pypi.org/project/fake-useragent/) При вызове метода из различных кусочков будет генерироваться рандомное сочетание операционной системы, спецификаций и версии браузера, которые можно передавать в запрос:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fake_useragent import UserAgent\n",
    "UserAgent().chrome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например, https://knowyourmeme.com/ не захочет пускать к себе python и выдаст ошибку 403. Она выдается сервером, если он доступен и способен обрабатывать запросы, но по некоторым личным причинам отказывается это делать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://knowyourmeme.com/'\n",
    "\n",
    "response = requests.get(url)\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А если сгенерировать User-Agent, вопросов у сервера не возникнет. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url, headers={'User-Agent': UserAgent().chrome})\n",
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Другой пример:__ если захотите спарсить ЦИАН, он начнет вам выдавать капчу. Один из вариантов обхода: менять ip через тор. Однако на практически каждый запрос из-под тора, ЦИАН будет выдавать капчу. Если добавить в запрос `User_Agent`, то капча будет вылезать намного реже. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## в) уходить глубже \n",
    "\n",
    "<center>\n",
    "<img src=\"https://raw.githubusercontent.com/hse-econ-data-science/eds_spring_2020/master/sem05_parsing/image/tor.jpg\" width=\"600\"> \n",
    "\n",
    "Можно попытаться обходить злые сервера через тор. Есть аж несколько способов, но мы про это говорить не будем. Лучше подробно почитать в нашей статье на Хабре. Ссылка на неё в конце тетрадки. Ещё в самом начале была. А ещё в середине [наверняка есть.](https://habr.com/ru/company/ods/blog/346632/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Совместить всё? \n",
    "\n",
    "1. Начните с малого \n",
    "2. Если продолжает банить, накидывайте новые примочки\n",
    "3. Каждая новая примочка бьёт по скорости \n",
    "4. [Разные примочки для requests](http://docs.python-requests.org/en/v0.10.6/user/advanced/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Хитрости: \n",
    "\n",
    "### Хитрость 1:  Не стесняйтесь пользоваться `try-except`\n",
    "\n",
    "Эта конструкция позволяет питону в случае ошибки сделать что-нибудь другое либо проигнорировать её. Например, мы хотим найти логарифм от всех чисел из списка: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log \n",
    "\n",
    "a = [1,2,3,-1,-5,10,3]\n",
    "\n",
    "for item in a:\n",
    "    print(log(item))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У нас не выходит, так как логарифм от отрицательных чисел не берётся. Чтобы код не падал при возникновении ошибки, мы можем его немного изменить: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log \n",
    "\n",
    "a = [1,2,3,-1,-5,10,3]\n",
    "\n",
    "for item in a:\n",
    "    try:\n",
    "        print(log(item))  # попробуй взять логарифм\n",
    "    except:\n",
    "        print('я не смог') # если не вышло, сознайся и работай дальше"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Как это использовать при парсинге?__  Интернет создаёт человек. У многих людей руки очень кривые. Предположим, что мы на ночь поставили парсер скачивать цены, он отработал час и упал из-за того, что на како-нибудь одной странице были криво проставлены теги, либо вылезло какое-то редкое поле, либо вылезли какие-то артефакты от старой версии сайта, которые не были учтены в нашем парсере. Гораздо лучше, чтобы код проигнорировал эту ошибку и продолжил работать дальше. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Ещё хитрости: \n",
    "\n",
    "* __Сохраняйте то, что парсите по мере скачки!__ Прямо внутрь цикла запихните код, который сохраняет файл! \n",
    "* Когда код упал в середине списка для скачки, не обязательно запускать его с самого начала. Просто сохраните тот кусок, который уже скачался и дозапустите код с места падения.\n",
    "* Засовывать цикл для обхода ссылок внутрь функции - не самая хорошая идея. Предположим, что надо обойти $100$ ссылок. Функция должна вернуть на выход объекты, которые скачались по всему этому добру. Она берёт и падает на $50$ объекте. Конечно же то, что уже было скачано, функция не возвращает. Всё, что вы накачали - вы теряете. Надо запускать заново. Почему? Потому что внутри функции своё пространство имён. Если бы вы делали это циклом влоб, то можно было бы сохранить первые $50$ объектов, которые уже лежат внутри листа, а потом продолжить скачку. \n",
    "* Можно ориентироваться на html-страничке с помощью `xpath`. Он предназначен для того, чтобы внутри html-странички можно было быстро находить какие-то элементы. [Подробнее можно почитать тут.](https://devhints.io/xpath)\n",
    "* Не ленитесь листать документацию. Из неё можно узнать много полезных штук. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Почиташки\n",
    "\n",
    "* [Парсим мемы в python](https://habr.com/ru/company/ods/blog/346632/) - подробная статья на Хабре, по которой можно научиться ... парсить (ВНЕЗАПНО) \n",
    "* [Тетрадки Ильи Щурова](https://github.com/ischurov/pythonhse) про python для анализа данных. В [лекции 9](https://nbviewer.jupyter.org/github/ischurov/pythonhse/blob/master/Lecture%209.ipynb) и [лекции 10](https://nbviewer.jupyter.org/github/ischurov/pythonhse/blob/master/Lecture%2010.ipynb) есть про парсеры. \n",
    "* [Книга про парсинг](https://github.com/FUlyankin/Parsers/blob/master/Ryan_Mitchell_Web_Scraping_with_Python-_Collecting_Data_from_the_Modern_Web_2015.pdf) на случай если вам совсем скучно и хочется почитать что-то длинное и на английском\n",
    "* [Продвинутое использование requests](https://2.python-requests.org/en/master/user/advanced/)\n",
    "* [Перевод документации по selenium на русский на хабре](https://habr.com/ru/post/248559/)\n",
    "\n",
    "\n",
    "* [Страничка с парсерами одного из семинаристов,](https://fulyankin.github.io/Parsers/) на ней много недописанного и кривого, но есть и интересное: \n",
    "    * [Более подробно про selenium](https://nbviewer.jupyter.org/github/FUlyankin/Parsers/blob/master/sems/3_Selenium_and_Tor/4.1%20Selenium%20.ipynb)\n",
    "    * [Немного устаревший гайд по парсинг вконтакте](https://nbviewer.jupyter.org/github/FUlyankin/ekanam_grand_research/blob/master/0.%20vk_parser_tutorial.ipynb)\n",
    "    * [Немного устаревший гайд про google maps](https://nbviewer.jupyter.org/github/FUlyankin/Parsers/blob/master/Parsers%20/Google_maps_API.ipynb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
